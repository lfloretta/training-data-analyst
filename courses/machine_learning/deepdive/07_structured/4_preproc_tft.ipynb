{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Preprocessing using tf.transform and Dataflow </h1>\n",
    "\n",
    "This notebook illustrates:\n",
    "<ol>\n",
    "<li> Creating datasets for Machine Learning using tf.transform and Dataflow\n",
    "</ol>\n",
    "<p>\n",
    "While Pandas is fine for experimenting, for operationalization of your workflow, it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam also allows for streaming.\n",
    "<p>\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform. So make sure to get a combo that is.\n",
    "* TFT 0.1.10 \n",
    "* TF 1.0 or higher\n",
    "* Apache Beam [GCP] 2.1.1 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform==0.4.0 apache-beam[gcp]==2.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "  warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "  import apache_beam as beam\n",
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'asl-ml-immersion-temp'\n",
    "PROJECT = 'asl-ml-immersion'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "if ! gsutil ls | grep -q gs://${BUCKET}/; then\n",
    "  gsutil mb -l ${REGION} gs://${BUCKET}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Save the query from earlier </h2>\n",
    "\n",
    "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother.  Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "SELECT\n",
    "  weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  FARM_FINGERPRINT(CONCAT(CAST(YEAR AS STRING), CAST(month AS STRING))) AS hashmonth\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE year > 2000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "df = bq.Query(query + \" LIMIT 100\").execute().result().to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create ML dataset using tf.transform and Dataflow </h2>\n",
    "<p>\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well.\n",
    "<p>\n",
    "Note that after you launch this, the notebook won't show you progress. Go to the GCP webconsole to the Dataflow section and monitor the running job. It took about <b>30 minutes</b> for me. If you wish to continue without doing this step, you can copy my preprocessed output:\n",
    "<pre>\n",
    "gsutil -m cp -r gs://asl-ml-immersion/babyweight/preproc_tft gs://your-bucket/\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# makes sure that the version of tensorflow and tensorflow_transform that we are using is on the worker machines\n",
    "echo \"tensorflow_transform==0.4.0\" > requirements.txt\n",
    "echo \"apache-beam[gcp]==2.2.0\" >> requirements.txt\n",
    "#pip freeze | grep tensorflow-transform > requirements.txt\n",
    "cat requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "with warnings.catch_warnings():\n",
    "  warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "  import apache_beam as beam\n",
    "  import tensorflow_transform as tft\n",
    "  from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "METADATA = {\n",
    "  'ORDERED_FIELDS' : ['weight_pounds', 'is_male', 'mother_age', 'plurality', 'gestation_weeks'],\n",
    "  'STR_FIELDS' : ['key', 'is_male', 'plurality'],\n",
    "  'FLT_FIELDS' : ['weight_pounds', 'mother_age', 'gestation_weeks']\n",
    "}\n",
    "\n",
    "def preprocess_tft(inputs):\n",
    "    import copy\n",
    "    import numpy as np\n",
    "\n",
    "    def center(x):\n",
    "          return x - tft.mean(x)\n",
    "\n",
    "    result = copy.copy(inputs) # shallow copy\n",
    "    result['mother_age_tft'] = center(inputs['mother_age'])\n",
    "    result['gestation_weeks_centered'] = tft.scale_to_0_1(inputs['gestation_weeks'])\n",
    "    return result\n",
    "\n",
    "\n",
    "def cleanup(rowdict, metadata):\n",
    "    import copy, hashlib\n",
    "    # create synthetic data where we assume that no ultrasound has been performed\n",
    "    # and so we don't know sex of the baby. Let's assume that we can tell the difference\n",
    "    # between single and multiple, but that the errors rates in determining exact number\n",
    "    # is difficult in the absence of an ultrasound.\n",
    "    #print(rowdict)\n",
    "    no_ultrasound = copy.deepcopy(rowdict)\n",
    "    no_ultrasound[u'is_male'] = u'Unknown'\n",
    "\n",
    "    if rowdict[u'plurality'] > 1:\n",
    "        no_ultrasound[u'plurality'] = u'Multiple(2+)'\n",
    "    else:\n",
    "        no_ultrasound[u'plurality'] = u'Single(1)'\n",
    "\n",
    "    w_ultrasound = copy.deepcopy(rowdict)\n",
    "    # Change the plurality column to strings\n",
    "    w_ultrasound[u'plurality'] = [u'Single(1)', u'Twins(2)',\n",
    "                                 u'Triplets(3)', u'Quadruplets(4)',\n",
    "                                 u'Quintuplets(5)'][rowdict[u'plurality']-1]\n",
    "\n",
    "    # add any missing columns, and correct the types\n",
    "    def tofloat(value, ifnot):\n",
    "      try:\n",
    "        return float(value)\n",
    "      except (ValueError, TypeError):\n",
    "        return ifnot\n",
    "\n",
    "    out_no_ultrasound = {\n",
    "      k : str(no_ultrasound[k]) if k in no_ultrasound else u'None' for k in metadata['STR_FIELDS']\n",
    "    }\n",
    "    out_no_ultrasound.update({\n",
    "        k : tofloat(no_ultrasound[k], -99) if k in no_ultrasound else -99 for k in metadata['FLT_FIELDS']\n",
    "    }\n",
    "    )\n",
    "\n",
    "    out_w_ultrasound = {\n",
    "      k : str(w_ultrasound[k]) if k in w_ultrasound else u'None' for k in metadata['STR_FIELDS']\n",
    "    }\n",
    "    out_w_ultrasound.update({\n",
    "        k : tofloat(w_ultrasound[k], -99) if k in w_ultrasound else -99 for k in metadata['FLT_FIELDS']\n",
    "    }\n",
    "    )\n",
    "\n",
    "    # cleanup: write out only the data we that we want to train on\n",
    "    # using no_ultrasound or w_ultrasound yield the same results\n",
    "    for out in [out_no_ultrasound, out_w_ultrasound]:\n",
    "      if (out['weight_pounds'] > 0 and\n",
    "              out['mother_age'] > 0 and\n",
    "              out['gestation_weeks'] > 0 ):\n",
    "\n",
    "        data = ','.join([str(out[k]) for k in metadata['ORDERED_FIELDS']])\n",
    "        out['key'] = hashlib.sha224(data).hexdigest()\n",
    "        yield out\n",
    "      \n",
    "def preprocess(query, in_test_mode):\n",
    "  import os\n",
    "  import os.path\n",
    "  import tempfile\n",
    "  from apache_beam.io import tfrecordio\n",
    "  from tensorflow_transform.coders import example_proto_coder\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "  job_name = 'preprocess-babyweight-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "  print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "  OUTPUT_DIR = 'gs://{0}/babyweight/preproc_tft/'.format(BUCKET)\n",
    "  import subprocess\n",
    "  subprocess.call('gsutil -m -o rm -r {}'.format(OUTPUT_DIR).split())\n",
    "\n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'region' : REGION,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'save_main_session': False,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "\n",
    "  RUNNER = 'DataflowRunner'\n",
    "\n",
    "  # set up metadata\n",
    "  raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in METADATA['STR_FIELDS']\n",
    "  }\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in METADATA['FLT_FIELDS']\n",
    "    })\n",
    "\n",
    "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "\n",
    "  def read_rawdata(p, step, test_mode):\n",
    "    if step == 'train':\n",
    "        selquery = 'SELECT * FROM ({}) WHERE MOD(ABS(hashmonth),4) < 3'.format(query)\n",
    "    else:\n",
    "        selquery = 'SELECT * FROM ({}) WHERE MOD(ABS(hashmonth),4) = 3'.format(query)\n",
    "\n",
    "    if test_mode:\n",
    "        selquery += ' LIMIT 1000'\n",
    "\n",
    "    #print 'Processing {} data from {}'.format(step, selquery)\n",
    "    return (p\n",
    "          | '{}_read'.format(step) >> beam.io.Read(\n",
    "                beam.io.BigQuerySource(query=selquery,\n",
    "                                       use_standard_sql=True),\n",
    "                                       )\n",
    "          | '{}_cleanup'.format(step) >> beam.FlatMap(cleanup, METADATA)\n",
    "         )\n",
    "\n",
    "  # run Beam\n",
    "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with beam_impl.Context(os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "      # analyze and transform training\n",
    "      raw_data = read_rawdata(p, 'train', in_test_mode)\n",
    "\n",
    "      # Combine data and schema into a dataset tuple.  Note that we already used\n",
    "      # the schema to \"read\" BigQuery data\n",
    "      raw_dataset = (raw_data, raw_data_metadata)\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "\n",
    "      _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'train'),\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "\n",
    "      # transform eval data\n",
    "      raw_test_data = read_rawdata(p, 'eval', in_test_mode)\n",
    "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "\n",
    "      # Don't need transformed data schema, it's the same as before.\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "\n",
    "      _ = transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'eval'),\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "\n",
    "\n",
    "      _ = (transform_fn\n",
    "           | 'WriteTransformFn' >>\n",
    "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata'))\n",
    "          )\n",
    "\n",
    "  job = p.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preprocess(query, in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "gsutil ls gs://${BUCKET}/babyweight/preproc_tft/*-00000*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2017 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
